{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "3a5afc4f-7540-4dce-8d18-ad74db6a22b7",
      "metadata": {
        "id": "3a5afc4f-7540-4dce-8d18-ad74db6a22b7"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoProcessor, AutoModelForCausalLM\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "import random\n",
        "import torch\n",
        "import json\n",
        "import copy\n",
        "import os\n",
        "import re\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "998b356b-630d-4b89-8139-1995e31822e7",
      "metadata": {
        "id": "998b356b-630d-4b89-8139-1995e31822e7"
      },
      "outputs": [],
      "source": [
        "model_id = 'microsoft/Florence-2-large'\n",
        "device = torch.device(\"cuda\")\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, torch_dtype='auto').eval().to(device)\n",
        "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "5427f95b-3c6e-4834-b08f-8af1a38306b7",
      "metadata": {
        "id": "5427f95b-3c6e-4834-b08f-8af1a38306b7"
      },
      "outputs": [],
      "source": [
        "def apply_filters(image, bboxes, labels):\n",
        "    \"\"\"\n",
        "    Filters detections based on criteria:\n",
        "    - Focuses on \"person\" and \"pet\" classes.\n",
        "    - Ensures the person is visible and occupies a reasonable portion of the image.\n",
        "    - Matches keywords exactly.\n",
        "\n",
        "    Parameters:\n",
        "    - image: PIL Image object\n",
        "    - bboxes: List of bounding boxes (x1, y1, x2, y2).\n",
        "    - labels: List of labels corresponding to the bounding boxes.\n",
        "\n",
        "    Returns:\n",
        "    - filtered_bboxes: Filtered bounding boxes.\n",
        "    - filtered_labels: Filtered labels.\n",
        "    \"\"\"\n",
        "    filtered_bboxes = []\n",
        "    filtered_labels = []\n",
        "    img_width, img_height = image.size\n",
        "\n",
        "    # Define exact keywords using regex with word boundaries\n",
        "    pet_keywords = re.compile(r'\\b(dog|cat)\\b', re.IGNORECASE)\n",
        "    person_keywords = re.compile(r'\\b(person|man|woman|boy|girl|child|kid)\\b', re.IGNORECASE)\n",
        "\n",
        "    for bbox, label in zip(bboxes, labels):\n",
        "        # Match exact keywords\n",
        "        if pet_keywords.search(label):\n",
        "            label = \"pet\"\n",
        "        elif person_keywords.search(label):\n",
        "            label = \"person\"\n",
        "        else:\n",
        "            continue  # Skip irrelevant labels\n",
        "\n",
        "        # Compute bounding box area\n",
        "        x1, y1, x2, y2 = bbox\n",
        "        bbox_width = x2 - x1\n",
        "        bbox_height = y2 - y1\n",
        "        bbox_area = bbox_width * bbox_height\n",
        "        img_area = img_width * img_height\n",
        "\n",
        "        # Filter by bounding box size (area between 5% and 95% of image area)\n",
        "        if not (0.05 * img_area <= bbox_area <= 0.95 * img_area):\n",
        "            continue\n",
        "\n",
        "        # Add filtered results\n",
        "        filtered_bboxes.append([x1, y1, x2, y2])\n",
        "        filtered_labels.append(label)\n",
        "\n",
        "    return filtered_bboxes, filtered_labels\n",
        "\n",
        "def plot_bbox(image, data):\n",
        "   # Create a figure and axes\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    # Display the image\n",
        "    ax.imshow(image)\n",
        "\n",
        "    # Plot each bounding box\n",
        "    for bbox, label in zip(data['bboxes'], data['labels']):\n",
        "        # Unpack the bounding box coordinates\n",
        "        x1, y1, x2, y2 = bbox\n",
        "        # Create a Rectangle patch\n",
        "        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=1, edgecolor='r', facecolor='none')\n",
        "        # Add the rectangle to the Axes\n",
        "        ax.add_patch(rect)\n",
        "        # Annotate the label\n",
        "        plt.text(x1, y1, label, color='white', fontsize=8, bbox=dict(facecolor='red', alpha=0.5))\n",
        "\n",
        "    # Remove the axis ticks and labels\n",
        "    ax.axis('off')\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "\n",
        "# Run example with Florence-2\n",
        "def run_example(task_prompt, image, text_input=None):\n",
        "    if text_input is None:\n",
        "        prompt = task_prompt\n",
        "    else:\n",
        "        prompt = task_prompt + text_input\n",
        "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device, torch.float16)\n",
        "    generated_ids = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"].cuda(),\n",
        "        pixel_values=inputs[\"pixel_values\"].cuda(),\n",
        "        max_new_tokens=1024,\n",
        "        early_stopping=False,\n",
        "        do_sample=False,\n",
        "        num_beams=3,\n",
        "    )\n",
        "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
        "    parsed_answer = processor.post_process_generation(\n",
        "        generated_text,\n",
        "        task=task_prompt,\n",
        "        image_size=(image.width, image.height)\n",
        "    )\n",
        "    return parsed_answer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd464a8e-0f93-465e-a58e-ec69fe739db7",
      "metadata": {
        "id": "dd464a8e-0f93-465e-a58e-ec69fe739db7"
      },
      "outputs": [],
      "source": [
        "flickr_dataset_path = \"flickr30k_images\"  \n",
        "image_paths = [str(path) for path in Path(flickr_dataset_path).rglob('*.jpg')]\n",
        "\n",
        "# Verify the dataset and load a sample image\n",
        "if len(image_paths) == 0:\n",
        "    print(\"No JPG images found in the specified path.\")\n",
        "else:\n",
        "    print(f\"Found {len(image_paths)} JPG images in the dataset.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef81b0c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the CSV file\n",
        "csv_path = \"flickr30k_descriptions.csv\"\n",
        "caption = pd.read_csv(csv_path, sep=r'\\|\\s*', engine='python', encoding='utf-8')\n",
        "\n",
        "# Convert 'comment' column to strings\n",
        "caption['comment'] = caption['comment'].astype(str)\n",
        "\n",
        "# Group comments by image_name and combine them into a single string\n",
        "caption = caption.groupby('image_name')['comment'].apply(lambda x: ' '.join(map(str, x))).reset_index()\n",
        "\n",
        "# Function to filter images based on inclusion and exclusion criteria\n",
        "def filter_images(caption, inclusion_keywords, exclusion_keywords=None, output_csv=None):\n",
        "    \"\"\"\n",
        "    Filters images based on inclusion and exclusion keywords.\n",
        "    \n",
        "    Parameters:\n",
        "    - caption: DataFrame with combined comments for each image.\n",
        "    - inclusion_keywords: List of keywords to include.\n",
        "    - exclusion_keywords: List of keywords to exclude (default: None).\n",
        "    - output_csv: CSV file to save filtered image names (default: None).\n",
        "\n",
        "    Returns:\n",
        "    - List of filtered image names.\n",
        "    \"\"\"\n",
        "    # Pre-compile regex patterns\n",
        "    inclusion_patterns = [re.compile(rf'\\b{keyword}\\b', re.IGNORECASE) for keyword in inclusion_keywords]\n",
        "    exclusion_patterns = (\n",
        "        [re.compile(rf'\\b{keyword}\\b', re.IGNORECASE) for keyword in exclusion_keywords]\n",
        "        if exclusion_keywords else []\n",
        "    )\n",
        "\n",
        "    filtered_images = []\n",
        "\n",
        "    # Iterate through caption\n",
        "    for _, row in caption.iterrows():\n",
        "        image_name = row['image_name']  # Image filename\n",
        "        combined_comments = row['comment']  # Combined comments for the image\n",
        "\n",
        "        # Check inclusion and exclusion criteria\n",
        "        contains_inclusion_keywords = any(pattern.search(combined_comments) for pattern in inclusion_patterns)\n",
        "        contains_exclusion_keywords = any(pattern.search(combined_comments) for pattern in exclusion_patterns)\n",
        "\n",
        "        if contains_inclusion_keywords and not contains_exclusion_keywords:\n",
        "            filtered_images.append(image_name)\n",
        "\n",
        "    # Save filtered image names to a CSV if specified\n",
        "    if output_csv:\n",
        "        pd.DataFrame({'image_name': filtered_images}).to_csv(output_csv, index=False)\n",
        "\n",
        "    return filtered_images\n",
        "\n",
        "\n",
        "# Filter for cats\n",
        "cat_images = filter_images(caption, inclusion_keywords=['cat'], output_csv=\"filtered_images_cats.csv\")\n",
        "print(f\"Relevant images identified for cats: {len(cat_images)}\")\n",
        "\n",
        "# Filter for dogs\n",
        "dog_images = filter_images(caption, inclusion_keywords=['dog'], output_csv=\"filtered_images_dogs.csv\")\n",
        "print(f\"Relevant images identified for dogs: {len(dog_images)}\")\n",
        "\n",
        "# Filter for persons with visible faces, excluding large groups\n",
        "person_images = filter_images(\n",
        "    caption,\n",
        "    inclusion_keywords=['face', 'posing', 'smile'],\n",
        "    exclusion_keywords=['four', 'five', 'six', 'seven', 'eight', 'nine', 'surrounded', 'many', 'several'],\n",
        "    output_csv=\"filtered_images_persons.csv\"\n",
        ")\n",
        "print(f\"Relevant images identified for persons excluding large groups: {len(person_images)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05f20c4d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load filtered image names from CSV\n",
        "filtered_images = pd.read_csv(\"filtered_images_cats.csv\")['image_name'].tolist()\n",
        "\n",
        "# Select a subset of images for visualization\n",
        "sample_images = filtered_images[:10]  # Adjust range as needed\n",
        "\n",
        "print(\"Processing images using <OD> with filters...\")\n",
        "\n",
        "for img_name in sample_images:\n",
        "    try:\n",
        "        # Load the image\n",
        "        image_path = f\"flickr30k_images/{img_name}\"  # Adjust to your dataset path\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        \n",
        "        # Step 1: Perform Object Detection\n",
        "        task_prompt = '<OD>'\n",
        "        od_results = run_example(task_prompt, image)\n",
        "        \n",
        "        # Extract bounding boxes and labels\n",
        "        bboxes = od_results['<OD>']['bboxes']\n",
        "        labels = od_results['<OD>']['labels']\n",
        "        \n",
        "        # Apply filters to refine results\n",
        "        filtered_bboxes, filtered_labels = apply_filters(image, bboxes, labels)\n",
        "        \n",
        "        # Combine filtered results for visualization\n",
        "        filtered_results = {'bboxes': filtered_bboxes, 'labels': filtered_labels}\n",
        "        \n",
        "        # Visualize the bounding boxes on the image\n",
        "        if filtered_bboxes:\n",
        "            print(f\"Visualizing filtered results for image: {img_name}\")\n",
        "            plot_bbox(image, filtered_results)\n",
        "        else:\n",
        "            print(f\"No relevant objects found in image: {img_name}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {img_name}: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d2c47c2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load filtered image names from CSV\n",
        "cat_images = pd.read_csv(\"filtered_images_cats.csv\")['image_name'].tolist()\n",
        "dog_images = pd.read_csv(\"filtered_images_dogs.csv\")['image_name'].tolist()\n",
        "person_images = pd.read_csv(\"filtered_images_persons.csv\")['image_name'].tolist()\n",
        "\n",
        "# Combine pet images (all cats + first 926 dogs to total 1000 pet images)\n",
        "pet_images = cat_images + dog_images[:926]\n",
        "person_images = person_images[:1000]  # First 1000 person images\n",
        "\n",
        "# Combine all selected images for processing\n",
        "selected_images = pet_images + person_images\n",
        "\n",
        "# Initialize counters and lists\n",
        "filtered_pets = []\n",
        "filtered_persons = []\n",
        "\n",
        "print(\"Processing images using <OD> with filters...\")\n",
        "\n",
        "for img_name in tqdm(selected_images, desc=\"Filtering images\", unit=\"image\"):\n",
        "    try:\n",
        "        # Load the image\n",
        "        image_path = f\"flickr30k_images/{img_name}\"  # Adjust to your dataset path\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        \n",
        "        # Step 1: Perform Object Detection\n",
        "        task_prompt = '<OD>'\n",
        "        od_results = run_example(task_prompt, image)\n",
        "        \n",
        "        # Extract bounding boxes and labels\n",
        "        bboxes = od_results['<OD>']['bboxes']\n",
        "        labels = od_results['<OD>']['labels']\n",
        "        \n",
        "        # Apply filters to refine results\n",
        "        filtered_bboxes, filtered_labels = apply_filters(image, bboxes, labels)\n",
        "        \n",
        "        # Count and collect filtered results\n",
        "        for label in filtered_labels:\n",
        "            if label == \"pet\":\n",
        "                filtered_pets.append(img_name)\n",
        "            elif label == \"person\":\n",
        "                filtered_persons.append(img_name)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {img_name}: {e}\")\n",
        "\n",
        "# Save results to CSV\n",
        "filtered_data = pd.DataFrame({\n",
        "    'image_name': filtered_pets + filtered_persons,\n",
        "    'label': ['pet'] * len(filtered_pets) + ['person'] * len(filtered_persons)\n",
        "})\n",
        "filtered_data.to_csv(\"filtered_final_images.csv\", index=False)\n",
        "\n",
        "# Print summary\n",
        "print(f\"Filtered pet images: {len(filtered_pets)}\")\n",
        "print(f\"Filtered person images: {len(filtered_persons)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7793f9af",
      "metadata": {},
      "outputs": [],
      "source": [
        "# OD\n",
        "# Load filtered images\n",
        "with open(\"combined_filtered_images.txt\", 'r') as f:\n",
        "    filtered_images = [line.strip() for line in f.readlines()]\n",
        "\n",
        "# Process images from the filtered list\n",
        "sample_images = filtered_images[350:360]  # Select images\n",
        "\n",
        "# Iterate through the sample images\n",
        "for img_path in sample_images:\n",
        "    try:\n",
        "        # Load the image\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        \n",
        "        # Use Florence-2 for Object Detection\n",
        "        task_prompt = '<OD>'  # Object Detection task\n",
        "        results = run_example(task_prompt, image)\n",
        "        \n",
        "        # Convert results to bounding box format\n",
        "        bbox_results = {\n",
        "            'bboxes': results['<OD>']['bboxes'],\n",
        "            'labels': results['<OD>']['labels']\n",
        "        }\n",
        "        \n",
        "        # Visualize the bounding boxes on the image\n",
        "        print(f\"Visualizing results for image: {img_path}\")\n",
        "        plot_bbox(image, bbox_results)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {img_path}: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e85fbf00",
      "metadata": {},
      "outputs": [],
      "source": [
        "# CAPTION + PHRASE GROUNDING\n",
        "# Load filtered images\n",
        "with open(\"combined_filtered_images.txt\", 'r') as f:\n",
        "    filtered_images = [line.strip() for line in f.readlines()]\n",
        "\n",
        "# Process images from the filtered list\n",
        "sample_images = filtered_images[350:360]  # Select a range of images\n",
        "\n",
        "print(\"Processing images using Cascaded Tasks (Caption + Phrase Grounding)...\")\n",
        "\n",
        "for img_path in sample_images:\n",
        "    try:\n",
        "        # Load the image\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        \n",
        "        # Step 1: Generate a caption for the image\n",
        "        task_prompt = '<CAPTION>'\n",
        "        caption_results = run_example(task_prompt, image)\n",
        "        caption = caption_results[task_prompt]  # Extract the generated caption\n",
        "        print(f\"Generated caption: {caption}\")\n",
        "        \n",
        "        # Step 2: Perform Phrase Grounding on the caption\n",
        "        task_prompt = '<CAPTION_TO_PHRASE_GROUNDING>'\n",
        "        grounding_results = run_example(task_prompt, image, text_input=caption)\n",
        "        \n",
        "        # Extract bounding boxes and labels\n",
        "        od_results = {\n",
        "            'bboxes': grounding_results['<CAPTION_TO_PHRASE_GROUNDING>']['bboxes'],\n",
        "            'labels': grounding_results['<CAPTION_TO_PHRASE_GROUNDING>']['labels']\n",
        "        }\n",
        "        \n",
        "        # Visualize the bounding boxes on the image\n",
        "        if od_results['bboxes']:\n",
        "            print(f\"Visualizing results for image: {img_path}\")\n",
        "            plot_bbox(image, od_results)\n",
        "        else:\n",
        "            print(f\"No relevant objects found in image: {img_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {img_path}: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7b3756c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# CAPTION + PHRASE GROUNDING + FILTER\n",
        "# Function to filter results based on custom criteria\n",
        "def apply_filters(image, bboxes, labels):\n",
        "    \"\"\"\n",
        "    Filters detections based on criteria:\n",
        "    - Focuses on \"person\" and \"pet\" classes.\n",
        "    - Ensures the person is visible and occupies a reasonable portion of the image.\n",
        "    \n",
        "    Parameters:\n",
        "    - image: PIL Image object\n",
        "    - bboxes: List of bounding boxes (x1, y1, x2, y2).\n",
        "    - labels: List of labels corresponding to the bounding boxes.\n",
        "\n",
        "    Returns:\n",
        "    - filtered_bboxes: Filtered bounding boxes.\n",
        "    - filtered_labels: Filtered labels.\n",
        "    \"\"\"\n",
        "    filtered_bboxes = []\n",
        "    filtered_labels = []\n",
        "    img_width, img_height = image.size\n",
        "\n",
        "    for bbox, label in zip(bboxes, labels):\n",
        "        # Standardize labels\n",
        "        if \"dog\" in label.lower() or \"cat\" in label.lower():\n",
        "            label = \"pet\"\n",
        "        elif any(x in label.lower() for x in [\"person\", \"man\", \"men\", \"woman\", \"women\", \"human\", \"boy\", \"girl\", \"child\", \"kid\"]):\n",
        "            label = \"person\"\n",
        "        else:\n",
        "            continue  # Skip irrelevant labels\n",
        "\n",
        "        # Compute bounding box area\n",
        "        x1, y1, x2, y2 = bbox\n",
        "        bbox_width = x2 - x1\n",
        "        bbox_height = y2 - y1\n",
        "        bbox_area = bbox_width * bbox_height\n",
        "        img_area = img_width * img_height\n",
        "         \n",
        "        # Filter by bounding box size (area between 10% and 90% of image area)\n",
        "        if not (0.05 * img_area <= bbox_area <= 0.95 * img_area):\n",
        "            continue\n",
        "\n",
        "        # Add filtered results\n",
        "        filtered_bboxes.append([x1, y1, x2, y2])\n",
        "        filtered_labels.append(label)\n",
        "\n",
        "    return filtered_bboxes, filtered_labels\n",
        "\n",
        "# Load filtered images\n",
        "with open(\"combined_filtered_images.txt\", 'r') as f:\n",
        "    filtered_images = [line.strip() for line in f.readlines()]\n",
        "\n",
        "# Process images using Cascaded Tasks with filters\n",
        "sample_images = filtered_images[350:360]  # Select a range of images\n",
        "\n",
        "print(\"Processing images using Cascaded Tasks with filters...\")\n",
        "\n",
        "for img_path in sample_images:\n",
        "    try:\n",
        "        # Load the image\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        \n",
        "        # Step 1: Generate a caption for the image\n",
        "        task_prompt = '<CAPTION>'\n",
        "        caption_results = run_example(task_prompt, image)\n",
        "        caption = caption_results[task_prompt]  # Extract the generated caption\n",
        "        print(f\"Generated caption: {caption}\")\n",
        "        \n",
        "        # Step 2: Perform Phrase Grounding on the caption\n",
        "        task_prompt = '<CAPTION_TO_PHRASE_GROUNDING>'\n",
        "        grounding_results = run_example(task_prompt, image, text_input=caption)\n",
        "        \n",
        "        # Extract bounding boxes and labels\n",
        "        bboxes = grounding_results['<CAPTION_TO_PHRASE_GROUNDING>']['bboxes']\n",
        "        labels = grounding_results['<CAPTION_TO_PHRASE_GROUNDING>']['labels']\n",
        "        \n",
        "        # Apply filters to refine results\n",
        "        filtered_bboxes, filtered_labels = apply_filters(image, bboxes, labels)\n",
        "        \n",
        "        # Combine filtered results for visualization\n",
        "        filtered_results = {'bboxes': filtered_bboxes, 'labels': filtered_labels}\n",
        "        \n",
        "        # Visualize the bounding boxes on the image\n",
        "        if filtered_bboxes:\n",
        "            print(f\"Visualizing filtered results for image: {img_path}\")\n",
        "            plot_bbox(image, filtered_results)\n",
        "        else:\n",
        "            print(f\"No relevant objects found in image: {img_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {img_path}: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a155d17",
      "metadata": {},
      "outputs": [],
      "source": [
        "# OD + FILTER\n",
        "# Function to filter results based on custom criteria\n",
        "def apply_filters(image, bboxes, labels):\n",
        "    \"\"\"\n",
        "    Filters detections based on criteria:\n",
        "    - Focuses on \"person\" and \"pet\" classes.\n",
        "    - Ensures the person is visible and occupies a reasonable portion of the image.\n",
        "    \n",
        "    Parameters:\n",
        "    - image: PIL Image object\n",
        "    - bboxes: List of bounding boxes (x1, y1, x2, y2).\n",
        "    - labels: List of labels corresponding to the bounding boxes.\n",
        "\n",
        "    Returns:\n",
        "    - filtered_bboxes: Filtered bounding boxes.\n",
        "    - filtered_labels: Filtered labels.\n",
        "    \"\"\"\n",
        "    filtered_bboxes = []\n",
        "    filtered_labels = []\n",
        "    img_width, img_height = image.size\n",
        "\n",
        "    for bbox, label in zip(bboxes, labels):\n",
        "        # Standardize labels\n",
        "        if \"dog\" in label.lower() or \"cat\" in label.lower():\n",
        "            label = \"pet\"\n",
        "        elif any(x in label.lower() for x in [\"person\", \"man\", \"woman\", \"boy\", \"girl\", \"child\", \"kid\"]):\n",
        "            label = \"person\"\n",
        "        else:\n",
        "            continue  # Skip irrelevant labels\n",
        "\n",
        "        # Compute bounding box area\n",
        "        x1, y1, x2, y2 = bbox\n",
        "        bbox_width = x2 - x1\n",
        "        bbox_height = y2 - y1\n",
        "        bbox_area = bbox_width * bbox_height\n",
        "        img_area = img_width * img_height\n",
        "         \n",
        "        # Filter by bounding box size (area between 10% and 90% of image area)\n",
        "        if not (0.05 * img_area <= bbox_area <= 0.95 * img_area):\n",
        "            continue\n",
        "\n",
        "        # Add filtered results\n",
        "        filtered_bboxes.append([x1, y1, x2, y2])\n",
        "        filtered_labels.append(label)\n",
        "\n",
        "    return filtered_bboxes, filtered_labels\n",
        "\n",
        "# Load filtered images\n",
        "with open(\"combined_filtered_images.txt\", 'r') as f:\n",
        "    filtered_images = [line.strip() for line in f.readlines()]\n",
        "\n",
        "# Process images using <OD> with filters\n",
        "sample_images = filtered_images[350:360]  # Select a range of images\n",
        "\n",
        "print(\"Processing images using <OD> with filters...\")\n",
        "\n",
        "for img_path in sample_images:\n",
        "    try:\n",
        "        # Load the image\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        \n",
        "        # Step 1: Perform Object Detection\n",
        "        task_prompt = '<OD>'\n",
        "        od_results = run_example(task_prompt, image)\n",
        "        \n",
        "        # Extract bounding boxes and labels\n",
        "        bboxes = od_results['<OD>']['bboxes']\n",
        "        labels = od_results['<OD>']['labels']\n",
        "        \n",
        "        # Apply filters to refine results\n",
        "        filtered_bboxes, filtered_labels = apply_filters(image, bboxes, labels)\n",
        "        \n",
        "        # Combine filtered results for visualization\n",
        "        filtered_results = {'bboxes': filtered_bboxes, 'labels': filtered_labels}\n",
        "        \n",
        "        # Visualize the bounding boxes on the image\n",
        "        if filtered_bboxes:\n",
        "            print(f\"Visualizing filtered results for image: {img_path}\")\n",
        "            plot_bbox(image, filtered_results)\n",
        "        else:\n",
        "            print(f\"No relevant objects found in image: {img_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {img_path}: {e}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "transformers-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
